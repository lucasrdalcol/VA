%% Assignment 1 - Autonomous Vehicles

clear 
close all
clc

% Load data from scenario
[allData, scenario, sensors] = scenario();

%% Use INS sensor to get position and velocities
t = [allData.Time]; %array with sample times

% Get position of the egocar with arrayfun
PP = cell2mat(arrayfun(@(S) S.INSMeasurements{1,1}.Position', allData, 'UniformOutput', false))';

% Plot graph for position through time
figure(1)
subplot(1,2,1)
plot(t, PP)
grid on
legend('x', 'y', 'z')
title('Ego Vehicle Position')
xlabel('t (s)')
ylabel('Position (m)')

% Get velocity of the car data
VV = cell2mat(arrayfun(@(S) S.INSMeasurements{1,1}.Velocity', allData, 'UniformOutput', false))';

% Plot graph for velocity through time
subplot(1,2,2)
plot(t, VV)
grid on
legend('v_x', 'v_y', 'v_z')
title('Ego Vehicle Velocity')
xlabel('t (s)')
ylabel('Velocity (m/s)')

Lcar = 0;
for n = 2:size(PP,1)
    ds = norm(PP(n, 1:2) - PP(n-1, 1:2));
    Lcar = Lcar + ds;
end

%% RADAR and Camera Detections

% Plot the car's trajectory map
figure(2)
plot(PP(:,1), PP(:,2), 'LineWidth', 5)
view(-90, 90)
grid on
axis equal
title("Map of the Car's trajectory")
xlabel('X (m)')
ylabel('Y (m)')
hold on

radarDetections = [];
cameraCar = [];
cameraTruck = [];
cameraBicycle = [];
cameraPedestrian = [];

% Cycle each time sample to get each object detection
for n = 1:numel(allData)
    objs = allData(n).ObjectDetections; % Object detections

    % Calculate the car transformation
    posCar = PP(n,:);
    orCar = [fliplr(allData(n).INSMeasurements{1,1}.Orientation)] * pi/180;
    TCarTrans = trvec2tform(posCar);
    TCarRot = eul2tform(orCar);
    TCar = TCarTrans * TCarRot;

    % Cycle each object detection
    for i = 1:numel(objs)
        obj = objs{i, 1};

        % Check the sensor the made the detection and plot the detections
        if obj.SensorIndex == 1 % RADAR Detection
           posObj = obj.Measurement(1:3)';
           orObj = obj.Measurement(4:6)' * pi/180;
           TObjTrans = trvec2tform(posObj);
           TObjRot = eul2tform(orObj);
           TObj = TObjTrans * TObjRot;
           posWorld = TCar * TObj * [0 0 0 1]';
           plot(posWorld(1), posWorld(2), 'ro')
           radarDetections = [radarDetections posWorld];

        elseif obj.SensorIndex == 2 % Camera Detection
           posObj = obj.Measurement(1:3)';
           orObj = obj.Measurement(4:6)' * pi/180;
           TObjTrans = trvec2tform(posObj);
           TObjRot = eul2tform(orObj);
           TObj = TObjTrans * TObjRot;
           posWorld = TCar * TObj * [0 0 0 1]';
           
           % Plot according to the classification
           switch obj.ObjectClassID
               case 1 % Car
                    plot(posWorld(1), posWorld(2), 'go')
                    cameraCar = [cameraCar posWorld(1:2)];
               case 2 % Truck
                    plot(posWorld(1), posWorld(2), 'bo')
                    cameraTruck = [cameraTruck posWorld(1:2)];
               case 3 % Bicycle
                    plot(posWorld(1), posWorld(2), 'co')
                    cameraBicycle = [cameraBicycle posWorld(1:2)];
               case 4 % Pedestrian
                    plot(posWorld(1), posWorld(2), 'mo')
                    cameraPedestrian = [cameraPedestrian posWorld(1:2)];
           end
%            legend("Car's trajectory", 'RADAR Detections', 'Camera Detection - car', 'Camera Detection - Bicycle', 'Camera Detection - pedestrian')
        end
    end
end

%% Count detections pedestrians and bicycles
distTreshold = 1;
numMinPoints = 2;

cameraCar_copy1 = cameraCar';
cameraBicycle_copy1 = cameraBicycle';
cameraPedestrian_copy1 = cameraPedestrian';
stopCars = 0;
Peds = 0;
Bikes = 0;

% For cars - it's not working well
for n = 1:size(cameraCar_copy1, 1)
    ds = cameraCar_copy1 - cameraCar_copy1(n, 1:2);
    ds = sqrt(ds(:, 1).^2 + ds(:, 2).^2);
 
    idxs = find(ds < distTreshold);
    
    
    if numel(idxs) >= numMinPoints
        cameraCar_copy1(idxs, :) = nan;
        stopCars = stopCars + 1;
    end
end

% For bicycles
for n = 1:size(cameraBicycle_copy1, 1)
    ds = cameraBicycle_copy1 - cameraBicycle_copy1(n, 1:2);
    ds = sqrt(ds(:, 1).^2 + ds(:, 2).^2);
 
    idxs = find(ds < distTreshold);
    
    
    if numel(idxs) >= numMinPoints
        cameraBicycle_copy1(idxs, :) = nan;
        Bikes = Bikes + 1;
    end
end

% For Pedestrians
for n = 1:size(cameraPedestrian_copy1, 1)
    ds = cameraPedestrian_copy1 - cameraPedestrian_copy1(n, 1:2);
    ds = sqrt(ds(:, 1).^2 + ds(:, 2).^2);
 
    idxs = find(ds < distTreshold);
    
    
    if numel(idxs) >= numMinPoints
        cameraPedestrian_copy1(idxs, :) = nan;
        Peds = Peds + 1;
    end
end

%% Using tracking from mathworks - https://www.mathworks.com/help/fusion/ug/extended-object-tracking.html

exPath = fullfile(matlabroot,'examples','driving_fusion','main');
addpath(exPath)
[scenario, egoVehicle, sensors] = helperCreateScenario;

% Create the display object
display = helperExtendedTargetTrackingDisplay;

% Create the Animation writer to record each frame of the figure for
% animation writing. Set 'RecordGIF' to true to enable GIF writing.
gifWriter = helperGIFWriter('Figure',display.Figure,...
    'RecordGIF',false);

% Function to return the errors given track and truth.
errorFcn = @(track,truth)helperExtendedTargetError(track,truth);

% Function to return the distance between track and truth.
distFcn = @(track,truth)helperExtendedTargetDistance(track,truth);

% Function to return the IDs from the ground truth. The default
% identifier assumes that the truth is identified with PlatformID. In
% drivingScenario, truth is identified with an ActorID.
truthIdFcn = @(x)[x.ActorID];

% Create metrics object.
tem = trackErrorMetrics(...
    'ErrorFunctionFormat','custom',...
    'EstimationErrorLabels',{'PositionError','VelocityError','DimensionsError','YawError'},...
    'EstimationErrorFcn',errorFcn,...
    'TruthIdentifierFcn',truthIdFcn);

tam = trackAssignmentMetrics(...
    'DistanceFunctionFormat','custom',...
    'AssignmentDistanceFcn',distFcn,...
    'DivergenceDistanceFcn',distFcn,...
    'TruthIdentifierFcn',truthIdFcn,...
    'AssignmentThreshold',30,...
    'DivergenceThreshold',35);

% Create ospa metric object.
tom = trackOSPAMetric(...
    'Distance','custom',...
    'DistanceFcn',distFcn,...
    'TruthIdentifierFcn',truthIdFcn);

trackerRunTimes = zeros(0,3);
ospaMetric = zeros(0,3);

% Create a multiObjectTracker
tracker = multiObjectTracker(...
    'FilterInitializationFcn', @helperInitPointFilter, ...
    'AssignmentThreshold', 30, ...
    'ConfirmationThreshold', [4 5], ...
    'DeletionThreshold', 3);

% Reset the random number generator for repeatable results
seed = 2018;
S = rng(seed);
timeStep = 1;

% For multiObjectTracker, the radar reports in Ego Cartesian frame and does
% not report velocity. This allows us to cluster detections from multiple
% sensors.

sensors{1}.HasRangeRate = false;
sensors{1}.DetectionCoordinates = 'Body';


while advance(scenario) && ishghandle(display.Figure)
    % Get the scenario time
    time = scenario.SimulationTime;

    % Collect detections from the ego vehicle sensors
    [detections,isValidTime] = helperDetect(sensors, egoVehicle, time);

    % Update the tracker if there are new detections
    if any(isValidTime)
        % Detections must be clustered first for the point tracker
        detectionClusters = helperClusterRadarDetections(detections);

        % Update the tracker
        tic
        % confirmedTracks are in scenario coordinates
        confirmedTracks = updateTracks(tracker, detectionClusters, time);
        t = toc;

        % Update the metrics
        % a. Obtain ground truth
        groundTruth = scenario.Actors(2:end); % All except Ego

        % b. Update assignment metrics
        tam(confirmedTracks,groundTruth);
        [trackIDs,truthIDs] = currentAssignment(tam);

        % c. Update error metrics
        tem(confirmedTracks,trackIDs,groundTruth,truthIDs);

        % d. Update ospa metric
        ospaMetric(timeStep,1) = tom(confirmedTracks, groundTruth);

        % Update bird's-eye-plot
        % Convert tracks to ego coordinates for display
        confirmedTracksEgo = helperConvertToEgoCoordinates(egoVehicle, confirmedTracks);
        display(egoVehicle, sensors, detections, confirmedTracksEgo, detectionClusters);
        drawnow;

        % Record tracker run times
        trackerRunTimes(timeStep,1) = t;
        timeStep = timeStep + 1;

        % Capture frames for animation
        gifWriter();
    end
end

% Capture the cumulative track metrics. The error metrics show the averaged
% value of the error over the simulation.
assignmentMetricsMOT = tam.trackMetricsTable;
errorMetricsMOT = tem.cumulativeTruthMetrics;

% Write GIF if requested
writeAnimation(gifWriter,'multiObjectTracking');

%% Lidar detections representation

% Definir os limites da zona a representar
xlimits = [-25 45]; %em metros
ylimits = [-25 45];
zlimits = [-20 20];
lidarViewer = pcplayer(xlimits, ylimits, zlimits); % permite representar um stream de nuvens de pontos 3D

% Definir os labels dos eixos
xlabel(lidarViewer.Axes, 'X (m)');
ylabel(lidarViewer.Axes, 'Y (m)');
zlabel(lidarViewer.Axes, 'Z (m)');

%Definir um colormap
colorLabels = [0      0.4470 0.7410;
               0.4660 0.6740 0.1880;
               0.9290 0.6940 0.1250;
               0.6350 0.0780 0.1840];

%Indexar as cores
colors.Unlabeled = 1; 
colors.Ground = 2;
colors.Ego = 3;
colors.Obstacle = 4;

vehicleDims = vehicleDimensions(); %4.7m de comprimento, 1.8m de largura, e 1.4m de altura

% Car limits for segmentation
tol = 1;
limits = tol * [-vehicleDims.Length/2 vehicleDims.Length/2;
                -vehicleDims.Width/2  vehicleDims.Width/2;
                -vehicleDims.Height   0];

%Aplicar o colormap ao eixo
colormap(lidarViewer.Axes, colorLabels);

minNumPoints = 50;

% Cycle each time sample to get each object detection
for n = 1:numel(allData)
    
    if ~isempty(allData(n).PointClouds.XLimits)
        ptCloud = allData(n).PointClouds;   
        
        points = struct();
        points.EgoPoints = ptCloud.Location(:,:,1) > limits(1,1) ...
                           & ptCloud.Location(:,:,1) < limits(1,2) ...
                           & ptCloud.Location(:,:,2) > limits(2,1) ...
                           & ptCloud.Location(:,:,2) < limits(2,2) ...
                           & ptCloud.Location(:,:,3) > limits(3,1) ...
                           & ptCloud.Location(:,:,3) < limits(3,2);
        
        scanSize = size(ptCloud.Location);
        scanSize = scanSize(1:2);
        
        %Criar um matriz que indique a cor a usar para cada ponto 32x1084
        colormapValues = ones(scanSize, 'like', ptCloud.Location) * colors.Unlabeled;
        
        %Aplicar a cor aos EgoPoints
        colormapValues(points.EgoPoints) = colors.Ego;
        
        points.GroundPoints = segmentGroundFromLidarData(ptCloud, 'ElevationAngleDelta', 0.5);
        
        points.GroundPoints = points.GroundPoints & ~points.EgoPoints; %Use only the points that are from the ground.
        % To do this, exclude the points of the car that were already detected.
        
        %Atualizar a matriz de índices de cor
        colormapValues(points.GroundPoints) = colors.Ground;
        
        % Get points without Ego and Ground points
        nonEgoGroundPoints = ~points.EgoPoints & ~points.GroundPoints;
        
        % Segment original point clouds with nonEgoGroundPoints
        ptCloudSegmented = select(ptCloud, nonEgoGroundPoints, 'Output', 'full');
        
        % Get a mask from origin to a distance of 40 m.
        points.ObstaclePoints = findNeighborsInRadius(ptCloudSegmented, [0 0 0], 40);
        
        % Segment point cloud for each obstacle 
        [labels, numClusters] = segmentLidarData(ptCloudSegmented, 1, 180, 'NumClusterPoints', minNumPoints);
        
        idxValidPoints = find(labels);
        labelColorIndex = labels(idxValidPoints);
        segmentedPtCloud = select(ptCloudSegmented, idxValidPoints);
        
        view(lidarViewer, segmentedPtCloud.Location, labelColorIndex) %Apresentar o plot
    end
end




